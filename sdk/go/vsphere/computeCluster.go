// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package vsphere

import (
	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/go/pulumi"
)

// -> **A note on the naming of this resource:** VMware refers to clusters of
// hosts in the UI and documentation as _clusters_, _HA clusters_, or _DRS
// clusters_. All of these refer to the same kind of resource (with the latter two
// referring to specific features of clustering). In Terraform, we use
// `vsphere_compute_cluster` to differentiate host clusters from _datastore
// clusters_, which are clusters of datastores that can be used to distribute load
// and ensure fault tolerance via distribution of virtual machines. Datastore
// clusters can also be managed through Terraform, via the
// [`vsphere_datastore_cluster` resource][docs-r-vsphere-datastore-cluster].
// 
// [docs-r-vsphere-datastore-cluster]: /docs/providers/vsphere/r/datastore_cluster.html
// 
// The `vsphere_compute_cluster` resource can be used to create and manage
// clusters of hosts allowing for resource control of compute resources, load
// balancing through DRS, and high availability through vSphere HA.
// 
// For more information on vSphere clusters and DRS, see [this
// page][ref-vsphere-drs-clusters]. For more information on vSphere HA, see [this
// page][ref-vsphere-ha-clusters].
// 
// [ref-vsphere-drs-clusters]: https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.resmgmt.doc/GUID-8ACF3502-5314-469F-8CC9-4A9BD5925BC2.html
// [ref-vsphere-ha-clusters]: https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.avail.doc/GUID-5432CA24-14F1-44E3-87FB-61D937831CF6.html
// 
// ~> **NOTE:** This resource requires vCenter and is not available on direct ESXi
// connections.
// 
// ~> **NOTE:** vSphere DRS requires a vSphere Enterprise Plus license.
type ComputeCluster struct {
	s *pulumi.ResourceState
}

// NewComputeCluster registers a new resource with the given unique name, arguments, and options.
func NewComputeCluster(ctx *pulumi.Context,
	name string, args *ComputeClusterArgs, opts ...pulumi.ResourceOpt) (*ComputeCluster, error) {
	if args == nil || args.DatacenterId == nil {
		return nil, errors.New("missing required argument 'DatacenterId'")
	}
	inputs := make(map[string]interface{})
	if args == nil {
		inputs["customAttributes"] = nil
		inputs["datacenterId"] = nil
		inputs["dpmAutomationLevel"] = nil
		inputs["dpmEnabled"] = nil
		inputs["dpmThreshold"] = nil
		inputs["drsAdvancedOptions"] = nil
		inputs["drsAutomationLevel"] = nil
		inputs["drsEnablePredictiveDrs"] = nil
		inputs["drsEnableVmOverrides"] = nil
		inputs["drsEnabled"] = nil
		inputs["drsMigrationThreshold"] = nil
		inputs["folder"] = nil
		inputs["forceEvacuateOnDestroy"] = nil
		inputs["haAdmissionControlFailoverHostSystemIds"] = nil
		inputs["haAdmissionControlHostFailureTolerance"] = nil
		inputs["haAdmissionControlPerformanceTolerance"] = nil
		inputs["haAdmissionControlPolicy"] = nil
		inputs["haAdmissionControlResourcePercentageAutoCompute"] = nil
		inputs["haAdmissionControlResourcePercentageCpu"] = nil
		inputs["haAdmissionControlResourcePercentageMemory"] = nil
		inputs["haAdmissionControlSlotPolicyExplicitCpu"] = nil
		inputs["haAdmissionControlSlotPolicyExplicitMemory"] = nil
		inputs["haAdmissionControlSlotPolicyUseExplicitSize"] = nil
		inputs["haAdvancedOptions"] = nil
		inputs["haDatastoreApdRecoveryAction"] = nil
		inputs["haDatastoreApdResponse"] = nil
		inputs["haDatastoreApdResponseDelay"] = nil
		inputs["haDatastorePdlResponse"] = nil
		inputs["haEnabled"] = nil
		inputs["haHeartbeatDatastoreIds"] = nil
		inputs["haHeartbeatDatastorePolicy"] = nil
		inputs["haHostIsolationResponse"] = nil
		inputs["haHostMonitoring"] = nil
		inputs["haVmComponentProtection"] = nil
		inputs["haVmDependencyRestartCondition"] = nil
		inputs["haVmFailureInterval"] = nil
		inputs["haVmMaximumFailureWindow"] = nil
		inputs["haVmMaximumResets"] = nil
		inputs["haVmMinimumUptime"] = nil
		inputs["haVmMonitoring"] = nil
		inputs["haVmRestartAdditionalDelay"] = nil
		inputs["haVmRestartPriority"] = nil
		inputs["haVmRestartTimeout"] = nil
		inputs["hostClusterExitTimeout"] = nil
		inputs["hostSystemIds"] = nil
		inputs["name"] = nil
		inputs["proactiveHaAutomationLevel"] = nil
		inputs["proactiveHaEnabled"] = nil
		inputs["proactiveHaModerateRemediation"] = nil
		inputs["proactiveHaProviderIds"] = nil
		inputs["proactiveHaSevereRemediation"] = nil
		inputs["tags"] = nil
	} else {
		inputs["customAttributes"] = args.CustomAttributes
		inputs["datacenterId"] = args.DatacenterId
		inputs["dpmAutomationLevel"] = args.DpmAutomationLevel
		inputs["dpmEnabled"] = args.DpmEnabled
		inputs["dpmThreshold"] = args.DpmThreshold
		inputs["drsAdvancedOptions"] = args.DrsAdvancedOptions
		inputs["drsAutomationLevel"] = args.DrsAutomationLevel
		inputs["drsEnablePredictiveDrs"] = args.DrsEnablePredictiveDrs
		inputs["drsEnableVmOverrides"] = args.DrsEnableVmOverrides
		inputs["drsEnabled"] = args.DrsEnabled
		inputs["drsMigrationThreshold"] = args.DrsMigrationThreshold
		inputs["folder"] = args.Folder
		inputs["forceEvacuateOnDestroy"] = args.ForceEvacuateOnDestroy
		inputs["haAdmissionControlFailoverHostSystemIds"] = args.HaAdmissionControlFailoverHostSystemIds
		inputs["haAdmissionControlHostFailureTolerance"] = args.HaAdmissionControlHostFailureTolerance
		inputs["haAdmissionControlPerformanceTolerance"] = args.HaAdmissionControlPerformanceTolerance
		inputs["haAdmissionControlPolicy"] = args.HaAdmissionControlPolicy
		inputs["haAdmissionControlResourcePercentageAutoCompute"] = args.HaAdmissionControlResourcePercentageAutoCompute
		inputs["haAdmissionControlResourcePercentageCpu"] = args.HaAdmissionControlResourcePercentageCpu
		inputs["haAdmissionControlResourcePercentageMemory"] = args.HaAdmissionControlResourcePercentageMemory
		inputs["haAdmissionControlSlotPolicyExplicitCpu"] = args.HaAdmissionControlSlotPolicyExplicitCpu
		inputs["haAdmissionControlSlotPolicyExplicitMemory"] = args.HaAdmissionControlSlotPolicyExplicitMemory
		inputs["haAdmissionControlSlotPolicyUseExplicitSize"] = args.HaAdmissionControlSlotPolicyUseExplicitSize
		inputs["haAdvancedOptions"] = args.HaAdvancedOptions
		inputs["haDatastoreApdRecoveryAction"] = args.HaDatastoreApdRecoveryAction
		inputs["haDatastoreApdResponse"] = args.HaDatastoreApdResponse
		inputs["haDatastoreApdResponseDelay"] = args.HaDatastoreApdResponseDelay
		inputs["haDatastorePdlResponse"] = args.HaDatastorePdlResponse
		inputs["haEnabled"] = args.HaEnabled
		inputs["haHeartbeatDatastoreIds"] = args.HaHeartbeatDatastoreIds
		inputs["haHeartbeatDatastorePolicy"] = args.HaHeartbeatDatastorePolicy
		inputs["haHostIsolationResponse"] = args.HaHostIsolationResponse
		inputs["haHostMonitoring"] = args.HaHostMonitoring
		inputs["haVmComponentProtection"] = args.HaVmComponentProtection
		inputs["haVmDependencyRestartCondition"] = args.HaVmDependencyRestartCondition
		inputs["haVmFailureInterval"] = args.HaVmFailureInterval
		inputs["haVmMaximumFailureWindow"] = args.HaVmMaximumFailureWindow
		inputs["haVmMaximumResets"] = args.HaVmMaximumResets
		inputs["haVmMinimumUptime"] = args.HaVmMinimumUptime
		inputs["haVmMonitoring"] = args.HaVmMonitoring
		inputs["haVmRestartAdditionalDelay"] = args.HaVmRestartAdditionalDelay
		inputs["haVmRestartPriority"] = args.HaVmRestartPriority
		inputs["haVmRestartTimeout"] = args.HaVmRestartTimeout
		inputs["hostClusterExitTimeout"] = args.HostClusterExitTimeout
		inputs["hostSystemIds"] = args.HostSystemIds
		inputs["name"] = args.Name
		inputs["proactiveHaAutomationLevel"] = args.ProactiveHaAutomationLevel
		inputs["proactiveHaEnabled"] = args.ProactiveHaEnabled
		inputs["proactiveHaModerateRemediation"] = args.ProactiveHaModerateRemediation
		inputs["proactiveHaProviderIds"] = args.ProactiveHaProviderIds
		inputs["proactiveHaSevereRemediation"] = args.ProactiveHaSevereRemediation
		inputs["tags"] = args.Tags
	}
	inputs["resourcePoolId"] = nil
	s, err := ctx.RegisterResource("vsphere:index/computeCluster:ComputeCluster", name, true, inputs, opts...)
	if err != nil {
		return nil, err
	}
	return &ComputeCluster{s: s}, nil
}

// GetComputeCluster gets an existing ComputeCluster resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetComputeCluster(ctx *pulumi.Context,
	name string, id pulumi.ID, state *ComputeClusterState, opts ...pulumi.ResourceOpt) (*ComputeCluster, error) {
	inputs := make(map[string]interface{})
	if state != nil {
		inputs["customAttributes"] = state.CustomAttributes
		inputs["datacenterId"] = state.DatacenterId
		inputs["dpmAutomationLevel"] = state.DpmAutomationLevel
		inputs["dpmEnabled"] = state.DpmEnabled
		inputs["dpmThreshold"] = state.DpmThreshold
		inputs["drsAdvancedOptions"] = state.DrsAdvancedOptions
		inputs["drsAutomationLevel"] = state.DrsAutomationLevel
		inputs["drsEnablePredictiveDrs"] = state.DrsEnablePredictiveDrs
		inputs["drsEnableVmOverrides"] = state.DrsEnableVmOverrides
		inputs["drsEnabled"] = state.DrsEnabled
		inputs["drsMigrationThreshold"] = state.DrsMigrationThreshold
		inputs["folder"] = state.Folder
		inputs["forceEvacuateOnDestroy"] = state.ForceEvacuateOnDestroy
		inputs["haAdmissionControlFailoverHostSystemIds"] = state.HaAdmissionControlFailoverHostSystemIds
		inputs["haAdmissionControlHostFailureTolerance"] = state.HaAdmissionControlHostFailureTolerance
		inputs["haAdmissionControlPerformanceTolerance"] = state.HaAdmissionControlPerformanceTolerance
		inputs["haAdmissionControlPolicy"] = state.HaAdmissionControlPolicy
		inputs["haAdmissionControlResourcePercentageAutoCompute"] = state.HaAdmissionControlResourcePercentageAutoCompute
		inputs["haAdmissionControlResourcePercentageCpu"] = state.HaAdmissionControlResourcePercentageCpu
		inputs["haAdmissionControlResourcePercentageMemory"] = state.HaAdmissionControlResourcePercentageMemory
		inputs["haAdmissionControlSlotPolicyExplicitCpu"] = state.HaAdmissionControlSlotPolicyExplicitCpu
		inputs["haAdmissionControlSlotPolicyExplicitMemory"] = state.HaAdmissionControlSlotPolicyExplicitMemory
		inputs["haAdmissionControlSlotPolicyUseExplicitSize"] = state.HaAdmissionControlSlotPolicyUseExplicitSize
		inputs["haAdvancedOptions"] = state.HaAdvancedOptions
		inputs["haDatastoreApdRecoveryAction"] = state.HaDatastoreApdRecoveryAction
		inputs["haDatastoreApdResponse"] = state.HaDatastoreApdResponse
		inputs["haDatastoreApdResponseDelay"] = state.HaDatastoreApdResponseDelay
		inputs["haDatastorePdlResponse"] = state.HaDatastorePdlResponse
		inputs["haEnabled"] = state.HaEnabled
		inputs["haHeartbeatDatastoreIds"] = state.HaHeartbeatDatastoreIds
		inputs["haHeartbeatDatastorePolicy"] = state.HaHeartbeatDatastorePolicy
		inputs["haHostIsolationResponse"] = state.HaHostIsolationResponse
		inputs["haHostMonitoring"] = state.HaHostMonitoring
		inputs["haVmComponentProtection"] = state.HaVmComponentProtection
		inputs["haVmDependencyRestartCondition"] = state.HaVmDependencyRestartCondition
		inputs["haVmFailureInterval"] = state.HaVmFailureInterval
		inputs["haVmMaximumFailureWindow"] = state.HaVmMaximumFailureWindow
		inputs["haVmMaximumResets"] = state.HaVmMaximumResets
		inputs["haVmMinimumUptime"] = state.HaVmMinimumUptime
		inputs["haVmMonitoring"] = state.HaVmMonitoring
		inputs["haVmRestartAdditionalDelay"] = state.HaVmRestartAdditionalDelay
		inputs["haVmRestartPriority"] = state.HaVmRestartPriority
		inputs["haVmRestartTimeout"] = state.HaVmRestartTimeout
		inputs["hostClusterExitTimeout"] = state.HostClusterExitTimeout
		inputs["hostSystemIds"] = state.HostSystemIds
		inputs["name"] = state.Name
		inputs["proactiveHaAutomationLevel"] = state.ProactiveHaAutomationLevel
		inputs["proactiveHaEnabled"] = state.ProactiveHaEnabled
		inputs["proactiveHaModerateRemediation"] = state.ProactiveHaModerateRemediation
		inputs["proactiveHaProviderIds"] = state.ProactiveHaProviderIds
		inputs["proactiveHaSevereRemediation"] = state.ProactiveHaSevereRemediation
		inputs["resourcePoolId"] = state.ResourcePoolId
		inputs["tags"] = state.Tags
	}
	s, err := ctx.ReadResource("vsphere:index/computeCluster:ComputeCluster", name, id, inputs, opts...)
	if err != nil {
		return nil, err
	}
	return &ComputeCluster{s: s}, nil
}

// URN is this resource's unique name assigned by Pulumi.
func (r *ComputeCluster) URN() *pulumi.URNOutput {
	return r.s.URN()
}

// ID is this resource's unique identifier assigned by its provider.
func (r *ComputeCluster) ID() *pulumi.IDOutput {
	return r.s.ID()
}

// A map of custom attribute ids to attribute
// value strings to set for the datastore cluster. See
// [here][docs-setting-custom-attributes] for a reference on how to set values
// for custom attributes.
func (r *ComputeCluster) CustomAttributes() *pulumi.MapOutput {
	return (*pulumi.MapOutput)(r.s.State["customAttributes"])
}

// The [managed object ID][docs-about-morefs] of
// the datacenter to create the cluster in. Forces a new resource if changed.
func (r *ComputeCluster) DatacenterId() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["datacenterId"])
}

// The automation level for host power
// operations in this cluster. Can be one of `manual` or `automated`. Default:
// `manual`.
func (r *ComputeCluster) DpmAutomationLevel() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["dpmAutomationLevel"])
}

// Enable DPM support for DRS in this cluster.
// Requires `drs_enabled` to be `true` in order to be effective.
// Default: `false`.
func (r *ComputeCluster) DpmEnabled() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["dpmEnabled"])
}

// A value between `1` and `5` indicating the
// threshold of load within the cluster that influences host power operations.
// This affects both power on and power off operations - a lower setting will
// tolerate more of a surplus/deficit than a higher setting. Default: `3`.
func (r *ComputeCluster) DpmThreshold() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["dpmThreshold"])
}

// A key/value map that specifies advanced
// options for DRS and DPM.
func (r *ComputeCluster) DrsAdvancedOptions() *pulumi.MapOutput {
	return (*pulumi.MapOutput)(r.s.State["drsAdvancedOptions"])
}

// The default automation level for all
// virtual machines in this cluster. Can be one of `manual`,
// `partiallyAutomated`, or `fullyAutomated`. Default: `manual`.
func (r *ComputeCluster) DrsAutomationLevel() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["drsAutomationLevel"])
}

// When `true`, enables DRS to use data
// from [vRealize Operations Manager][ref-vsphere-vro] to make proactive DRS
// recommendations. <sup>\*</sup>
func (r *ComputeCluster) DrsEnablePredictiveDrs() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["drsEnablePredictiveDrs"])
}

// Allow individual DRS overrides to be
// set for virtual machines in the cluster. Default: `true`.
func (r *ComputeCluster) DrsEnableVmOverrides() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["drsEnableVmOverrides"])
}

// Enable DRS for this cluster. Default: `false`.
func (r *ComputeCluster) DrsEnabled() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["drsEnabled"])
}

// A value between `1` and `5` indicating
// the threshold of imbalance tolerated between hosts. A lower setting will
// tolerate more imbalance while a higher setting will tolerate less. Default:
// `3`.
func (r *ComputeCluster) DrsMigrationThreshold() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["drsMigrationThreshold"])
}

// The relative path to a folder to put this cluster in.
// This is a path relative to the datacenter you are deploying the cluster to.
// Example: for the `dc1` datacenter, and a provided `folder` of `foo/bar`,
// Terraform will place a cluster named `terraform-compute-cluster-test` in a
// host folder located at `/dc1/host/foo/bar`, with the final inventory path
// being `/dc1/host/foo/bar/terraform-datastore-cluster-test`.
func (r *ComputeCluster) Folder() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["folder"])
}

// When destroying the resource, setting this to
// `true` will auto-remove any hosts that are currently a member of the cluster,
// as if they were removed by taking their entry out of `host_system_ids` (see
// below). This is an advanced
// option and should only be used for testing. Default: `false`.
func (r *ComputeCluster) ForceEvacuateOnDestroy() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["forceEvacuateOnDestroy"])
}

// Defines the
// [managed object IDs][docs-about-morefs] of hosts to use as dedicated failover
// hosts. These hosts are kept as available as possible - admission control will
// block access to the host, and DRS will ignore the host when making
// recommendations.
func (r *ComputeCluster) HaAdmissionControlFailoverHostSystemIds() *pulumi.ArrayOutput {
	return (*pulumi.ArrayOutput)(r.s.State["haAdmissionControlFailoverHostSystemIds"])
}

// The maximum number
// of failed hosts that admission control tolerates when making decisions on
// whether to permit virtual machine operations. The maximum is one less than
// the number of hosts in the cluster. Default: `1`.
// <sup>\*</sup>
func (r *ComputeCluster) HaAdmissionControlHostFailureTolerance() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlHostFailureTolerance"])
}

// The percentage of
// resource reduction that a cluster of virtual machines can tolerate in case of
// a failover. A value of 0 produces warnings only, whereas a value of 100
// disables the setting. Default: `100` (disabled).
func (r *ComputeCluster) HaAdmissionControlPerformanceTolerance() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlPerformanceTolerance"])
}

// The type of admission control
// policy to use with vSphere HA. Can be one of `resourcePercentage`,
// `slotPolicy`, `failoverHosts`, or `disabled`. Default: `resourcePercentage`.
func (r *ComputeCluster) HaAdmissionControlPolicy() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haAdmissionControlPolicy"])
}

// 
// Automatically determine available resource percentages by subtracting the
// average number of host resources represented by the
// `ha_admission_control_host_failure_tolerance`
// setting from the total amount of resources in the cluster. Disable to supply
// user-defined values. Default: `true`.
// <sup>\*</sup>
func (r *ComputeCluster) HaAdmissionControlResourcePercentageAutoCompute() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["haAdmissionControlResourcePercentageAutoCompute"])
}

// Controls the
// user-defined percentage of CPU resources in the cluster to reserve for
// failover. Default: `100`.
func (r *ComputeCluster) HaAdmissionControlResourcePercentageCpu() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlResourcePercentageCpu"])
}

// Controls the
// user-defined percentage of memory resources in the cluster to reserve for
// failover. Default: `100`.
func (r *ComputeCluster) HaAdmissionControlResourcePercentageMemory() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlResourcePercentageMemory"])
}

// Controls the
// user-defined CPU slot size, in MHz. Default: `32`.
func (r *ComputeCluster) HaAdmissionControlSlotPolicyExplicitCpu() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlSlotPolicyExplicitCpu"])
}

// Controls the
// user-defined memory slot size, in MB. Default: `100`.
func (r *ComputeCluster) HaAdmissionControlSlotPolicyExplicitMemory() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haAdmissionControlSlotPolicyExplicitMemory"])
}

// Controls
// whether or not you wish to supply explicit values to CPU and memory slot
// sizes. The default is `false`, which tells vSphere to gather a automatic
// average based on all powered-on virtual machines currently in the cluster.
func (r *ComputeCluster) HaAdmissionControlSlotPolicyUseExplicitSize() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["haAdmissionControlSlotPolicyUseExplicitSize"])
}

// A key/value map that specifies advanced
// options for vSphere HA.
func (r *ComputeCluster) HaAdvancedOptions() *pulumi.MapOutput {
	return (*pulumi.MapOutput)(r.s.State["haAdvancedOptions"])
}

// Controls the action to take
// on virtual machines if an APD status on an affected datastore clears in the
// middle of an APD event. Can be one of `none` or `reset`. Default: `none`.
// <sup>\*</sup>
func (r *ComputeCluster) HaDatastoreApdRecoveryAction() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haDatastoreApdRecoveryAction"])
}

// Controls the action to take on
// virtual machines when the cluster has detected loss to all paths to a
// relevant datastore. Can be one of `disabled`, `warning`,
// `restartConservative`, or `restartAggressive`.  Default: `disabled`.
// <sup>\*</sup>
func (r *ComputeCluster) HaDatastoreApdResponse() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haDatastoreApdResponse"])
}

// Controls the delay in minutes
// to wait after an APD timeout event to execute the response action defined in
// `ha_datastore_apd_response`. Default: `3`
// minutes. <sup>\*</sup>
func (r *ComputeCluster) HaDatastoreApdResponseDelay() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haDatastoreApdResponseDelay"])
}

// Controls the action to take on
// virtual machines when the cluster has detected a permanent device loss to a
// relevant datastore. Can be one of `disabled`, `warning`, or
// `restartAggressive`. Default: `disabled`.
// <sup>\*</sup>
func (r *ComputeCluster) HaDatastorePdlResponse() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haDatastorePdlResponse"])
}

// Enable vSphere HA for this cluster. Default:
// `false`.
func (r *ComputeCluster) HaEnabled() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["haEnabled"])
}

// The list of managed object IDs for
// preferred datastores to use for HA heartbeating. This setting is only useful
// when `ha_heartbeat_datastore_policy` is set
// to either `userSelectedDs` or `allFeasibleDsWithUserPreference`.
func (r *ComputeCluster) HaHeartbeatDatastoreIds() *pulumi.ArrayOutput {
	return (*pulumi.ArrayOutput)(r.s.State["haHeartbeatDatastoreIds"])
}

// The selection policy for HA
// heartbeat datastores. Can be one of `allFeasibleDs`, `userSelectedDs`, or
// `allFeasibleDsWithUserPreference`. Default:
// `allFeasibleDsWithUserPreference`.
func (r *ComputeCluster) HaHeartbeatDatastorePolicy() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haHeartbeatDatastorePolicy"])
}

// The action to take on virtual
// machines when a host has detected that it has been isolated from the rest of
// the cluster. Can be one of `none`, `powerOff`, or `shutdown`. Default:
// `none`.
func (r *ComputeCluster) HaHostIsolationResponse() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haHostIsolationResponse"])
}

// Global setting that controls whether
// vSphere HA remediates virtual machines on host failure. Can be one of `enabled`
// or `disabled`. Default: `enabled`.
func (r *ComputeCluster) HaHostMonitoring() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haHostMonitoring"])
}

// Controls vSphere VM component
// protection for virtual machines in this cluster. Can be one of `enabled` or
// `disabled`. Default: `enabled`.
// <sup>\*</sup>
func (r *ComputeCluster) HaVmComponentProtection() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haVmComponentProtection"])
}

// The condition used to
// determine whether or not virtual machines in a certain restart priority class
// are online, allowing HA to move on to restarting virtual machines on the next
// priority. Can be one of `none`, `poweredOn`, `guestHbStatusGreen`, or
// `appHbStatusGreen`. The default is `none`, which means that a virtual machine
// is considered ready immediately after a host is found to start it on.
// <sup>\*</sup>
func (r *ComputeCluster) HaVmDependencyRestartCondition() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haVmDependencyRestartCondition"])
}

// If a heartbeat from a virtual machine
// is not received within this configured interval, the virtual machine is
// marked as failed. The value is in seconds. Default: `30`.
func (r *ComputeCluster) HaVmFailureInterval() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmFailureInterval"])
}

// The length of the reset window in
// which `ha_vm_maximum_resets` can operate. When this
// window expires, no more resets are attempted regardless of the setting
// configured in `ha_vm_maximum_resets`. `-1` means no window, meaning an
// unlimited reset time is allotted. The value is specified in seconds. Default:
// `-1` (no window).
func (r *ComputeCluster) HaVmMaximumFailureWindow() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmMaximumFailureWindow"])
}

// The maximum number of resets that HA will
// perform to a virtual machine when responding to a failure event. Default: `3`
func (r *ComputeCluster) HaVmMaximumResets() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmMaximumResets"])
}

// The time, in seconds, that HA waits after
// powering on a virtual machine before monitoring for heartbeats. Default:
// `120` (2 minutes).
func (r *ComputeCluster) HaVmMinimumUptime() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmMinimumUptime"])
}

// The type of virtual machine monitoring to use
// when HA is enabled in the cluster. Can be one of `vmMonitoringDisabled`,
// `vmMonitoringOnly`, or `vmAndAppMonitoring`. Default: `vmMonitoringDisabled`.
func (r *ComputeCluster) HaVmMonitoring() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haVmMonitoring"])
}

// Additional delay in seconds
// after ready condition is met. A VM is considered ready at this point.
// Default: `0` (no delay). <sup>\*</sup>
func (r *ComputeCluster) HaVmRestartAdditionalDelay() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmRestartAdditionalDelay"])
}

// The default restart priority
// for affected virtual machines when vSphere detects a host failure. Can be one
// of `lowest`, `low`, `medium`, `high`, or `highest`. Default: `medium`.
func (r *ComputeCluster) HaVmRestartPriority() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["haVmRestartPriority"])
}

// The maximum time, in seconds,
// that vSphere HA will wait for virtual machines in one priority to be ready
// before proceeding with the next priority. Default: `600` (10 minutes).
// <sup>\*</sup>
func (r *ComputeCluster) HaVmRestartTimeout() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["haVmRestartTimeout"])
}

// The timeout for each host maintenance mode
// operation when removing hosts from a cluster. The value is specified in
// seconds. Default: `3600` (1 hour).
func (r *ComputeCluster) HostClusterExitTimeout() *pulumi.IntOutput {
	return (*pulumi.IntOutput)(r.s.State["hostClusterExitTimeout"])
}

// The [managed object IDs][docs-about-morefs] of
// the hosts to put in the cluster.
func (r *ComputeCluster) HostSystemIds() *pulumi.ArrayOutput {
	return (*pulumi.ArrayOutput)(r.s.State["hostSystemIds"])
}

// The name of the cluster.
func (r *ComputeCluster) Name() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["name"])
}

// Determines how the host
// quarantine, maintenance mode, or virtual machine migration recommendations
// made by proactive HA are to be handled. Can be one of `Automated` or
// `Manual`. Default: `Manual`. <sup>\*</sup>
func (r *ComputeCluster) ProactiveHaAutomationLevel() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["proactiveHaAutomationLevel"])
}

// Enables Proactive HA. Default: `false`.
// <sup>\*</sup>
func (r *ComputeCluster) ProactiveHaEnabled() *pulumi.BoolOutput {
	return (*pulumi.BoolOutput)(r.s.State["proactiveHaEnabled"])
}

// The configured remediation
// for moderately degraded hosts. Can be one of `MaintenanceMode` or
// `QuarantineMode`. Note that this cannot be set to `MaintenanceMode` when
// `proactive_ha_severe_remediation` is set
// to `QuarantineMode`. Default: `QuarantineMode`.
// <sup>\*</sup>
func (r *ComputeCluster) ProactiveHaModerateRemediation() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["proactiveHaModerateRemediation"])
}

// The list of IDs for health update
// providers configured for this cluster.
// <sup>\*</sup>
func (r *ComputeCluster) ProactiveHaProviderIds() *pulumi.ArrayOutput {
	return (*pulumi.ArrayOutput)(r.s.State["proactiveHaProviderIds"])
}

// The configured remediation for
// severely degraded hosts. Can be one of `MaintenanceMode` or `QuarantineMode`.
// Note that this cannot be set to `QuarantineMode` when
// `proactive_ha_moderate_remediation` is
// set to `MaintenanceMode`. Default: `QuarantineMode`.
// <sup>\*</sup>
func (r *ComputeCluster) ProactiveHaSevereRemediation() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["proactiveHaSevereRemediation"])
}

// The managed object ID of the cluster's root resource pool.
func (r *ComputeCluster) ResourcePoolId() *pulumi.StringOutput {
	return (*pulumi.StringOutput)(r.s.State["resourcePoolId"])
}

// The IDs of any tags to attach to this resource. See
// [here][docs-applying-tags] for a reference on how to apply tags.
func (r *ComputeCluster) Tags() *pulumi.ArrayOutput {
	return (*pulumi.ArrayOutput)(r.s.State["tags"])
}

// Input properties used for looking up and filtering ComputeCluster resources.
type ComputeClusterState struct {
	// A map of custom attribute ids to attribute
	// value strings to set for the datastore cluster. See
	// [here][docs-setting-custom-attributes] for a reference on how to set values
	// for custom attributes.
	CustomAttributes interface{}
	// The [managed object ID][docs-about-morefs] of
	// the datacenter to create the cluster in. Forces a new resource if changed.
	DatacenterId interface{}
	// The automation level for host power
	// operations in this cluster. Can be one of `manual` or `automated`. Default:
	// `manual`.
	DpmAutomationLevel interface{}
	// Enable DPM support for DRS in this cluster.
	// Requires `drs_enabled` to be `true` in order to be effective.
	// Default: `false`.
	DpmEnabled interface{}
	// A value between `1` and `5` indicating the
	// threshold of load within the cluster that influences host power operations.
	// This affects both power on and power off operations - a lower setting will
	// tolerate more of a surplus/deficit than a higher setting. Default: `3`.
	DpmThreshold interface{}
	// A key/value map that specifies advanced
	// options for DRS and DPM.
	DrsAdvancedOptions interface{}
	// The default automation level for all
	// virtual machines in this cluster. Can be one of `manual`,
	// `partiallyAutomated`, or `fullyAutomated`. Default: `manual`.
	DrsAutomationLevel interface{}
	// When `true`, enables DRS to use data
	// from [vRealize Operations Manager][ref-vsphere-vro] to make proactive DRS
	// recommendations. <sup>\*</sup>
	DrsEnablePredictiveDrs interface{}
	// Allow individual DRS overrides to be
	// set for virtual machines in the cluster. Default: `true`.
	DrsEnableVmOverrides interface{}
	// Enable DRS for this cluster. Default: `false`.
	DrsEnabled interface{}
	// A value between `1` and `5` indicating
	// the threshold of imbalance tolerated between hosts. A lower setting will
	// tolerate more imbalance while a higher setting will tolerate less. Default:
	// `3`.
	DrsMigrationThreshold interface{}
	// The relative path to a folder to put this cluster in.
	// This is a path relative to the datacenter you are deploying the cluster to.
	// Example: for the `dc1` datacenter, and a provided `folder` of `foo/bar`,
	// Terraform will place a cluster named `terraform-compute-cluster-test` in a
	// host folder located at `/dc1/host/foo/bar`, with the final inventory path
	// being `/dc1/host/foo/bar/terraform-datastore-cluster-test`.
	Folder interface{}
	// When destroying the resource, setting this to
	// `true` will auto-remove any hosts that are currently a member of the cluster,
	// as if they were removed by taking their entry out of `host_system_ids` (see
	// below). This is an advanced
	// option and should only be used for testing. Default: `false`.
	ForceEvacuateOnDestroy interface{}
	// Defines the
	// [managed object IDs][docs-about-morefs] of hosts to use as dedicated failover
	// hosts. These hosts are kept as available as possible - admission control will
	// block access to the host, and DRS will ignore the host when making
	// recommendations.
	HaAdmissionControlFailoverHostSystemIds interface{}
	// The maximum number
	// of failed hosts that admission control tolerates when making decisions on
	// whether to permit virtual machine operations. The maximum is one less than
	// the number of hosts in the cluster. Default: `1`.
	// <sup>\*</sup>
	HaAdmissionControlHostFailureTolerance interface{}
	// The percentage of
	// resource reduction that a cluster of virtual machines can tolerate in case of
	// a failover. A value of 0 produces warnings only, whereas a value of 100
	// disables the setting. Default: `100` (disabled).
	HaAdmissionControlPerformanceTolerance interface{}
	// The type of admission control
	// policy to use with vSphere HA. Can be one of `resourcePercentage`,
	// `slotPolicy`, `failoverHosts`, or `disabled`. Default: `resourcePercentage`.
	HaAdmissionControlPolicy interface{}
	// 
	// Automatically determine available resource percentages by subtracting the
	// average number of host resources represented by the
	// `ha_admission_control_host_failure_tolerance`
	// setting from the total amount of resources in the cluster. Disable to supply
	// user-defined values. Default: `true`.
	// <sup>\*</sup>
	HaAdmissionControlResourcePercentageAutoCompute interface{}
	// Controls the
	// user-defined percentage of CPU resources in the cluster to reserve for
	// failover. Default: `100`.
	HaAdmissionControlResourcePercentageCpu interface{}
	// Controls the
	// user-defined percentage of memory resources in the cluster to reserve for
	// failover. Default: `100`.
	HaAdmissionControlResourcePercentageMemory interface{}
	// Controls the
	// user-defined CPU slot size, in MHz. Default: `32`.
	HaAdmissionControlSlotPolicyExplicitCpu interface{}
	// Controls the
	// user-defined memory slot size, in MB. Default: `100`.
	HaAdmissionControlSlotPolicyExplicitMemory interface{}
	// Controls
	// whether or not you wish to supply explicit values to CPU and memory slot
	// sizes. The default is `false`, which tells vSphere to gather a automatic
	// average based on all powered-on virtual machines currently in the cluster.
	HaAdmissionControlSlotPolicyUseExplicitSize interface{}
	// A key/value map that specifies advanced
	// options for vSphere HA.
	HaAdvancedOptions interface{}
	// Controls the action to take
	// on virtual machines if an APD status on an affected datastore clears in the
	// middle of an APD event. Can be one of `none` or `reset`. Default: `none`.
	// <sup>\*</sup>
	HaDatastoreApdRecoveryAction interface{}
	// Controls the action to take on
	// virtual machines when the cluster has detected loss to all paths to a
	// relevant datastore. Can be one of `disabled`, `warning`,
	// `restartConservative`, or `restartAggressive`.  Default: `disabled`.
	// <sup>\*</sup>
	HaDatastoreApdResponse interface{}
	// Controls the delay in minutes
	// to wait after an APD timeout event to execute the response action defined in
	// `ha_datastore_apd_response`. Default: `3`
	// minutes. <sup>\*</sup>
	HaDatastoreApdResponseDelay interface{}
	// Controls the action to take on
	// virtual machines when the cluster has detected a permanent device loss to a
	// relevant datastore. Can be one of `disabled`, `warning`, or
	// `restartAggressive`. Default: `disabled`.
	// <sup>\*</sup>
	HaDatastorePdlResponse interface{}
	// Enable vSphere HA for this cluster. Default:
	// `false`.
	HaEnabled interface{}
	// The list of managed object IDs for
	// preferred datastores to use for HA heartbeating. This setting is only useful
	// when `ha_heartbeat_datastore_policy` is set
	// to either `userSelectedDs` or `allFeasibleDsWithUserPreference`.
	HaHeartbeatDatastoreIds interface{}
	// The selection policy for HA
	// heartbeat datastores. Can be one of `allFeasibleDs`, `userSelectedDs`, or
	// `allFeasibleDsWithUserPreference`. Default:
	// `allFeasibleDsWithUserPreference`.
	HaHeartbeatDatastorePolicy interface{}
	// The action to take on virtual
	// machines when a host has detected that it has been isolated from the rest of
	// the cluster. Can be one of `none`, `powerOff`, or `shutdown`. Default:
	// `none`.
	HaHostIsolationResponse interface{}
	// Global setting that controls whether
	// vSphere HA remediates virtual machines on host failure. Can be one of `enabled`
	// or `disabled`. Default: `enabled`.
	HaHostMonitoring interface{}
	// Controls vSphere VM component
	// protection for virtual machines in this cluster. Can be one of `enabled` or
	// `disabled`. Default: `enabled`.
	// <sup>\*</sup>
	HaVmComponentProtection interface{}
	// The condition used to
	// determine whether or not virtual machines in a certain restart priority class
	// are online, allowing HA to move on to restarting virtual machines on the next
	// priority. Can be one of `none`, `poweredOn`, `guestHbStatusGreen`, or
	// `appHbStatusGreen`. The default is `none`, which means that a virtual machine
	// is considered ready immediately after a host is found to start it on.
	// <sup>\*</sup>
	HaVmDependencyRestartCondition interface{}
	// If a heartbeat from a virtual machine
	// is not received within this configured interval, the virtual machine is
	// marked as failed. The value is in seconds. Default: `30`.
	HaVmFailureInterval interface{}
	// The length of the reset window in
	// which `ha_vm_maximum_resets` can operate. When this
	// window expires, no more resets are attempted regardless of the setting
	// configured in `ha_vm_maximum_resets`. `-1` means no window, meaning an
	// unlimited reset time is allotted. The value is specified in seconds. Default:
	// `-1` (no window).
	HaVmMaximumFailureWindow interface{}
	// The maximum number of resets that HA will
	// perform to a virtual machine when responding to a failure event. Default: `3`
	HaVmMaximumResets interface{}
	// The time, in seconds, that HA waits after
	// powering on a virtual machine before monitoring for heartbeats. Default:
	// `120` (2 minutes).
	HaVmMinimumUptime interface{}
	// The type of virtual machine monitoring to use
	// when HA is enabled in the cluster. Can be one of `vmMonitoringDisabled`,
	// `vmMonitoringOnly`, or `vmAndAppMonitoring`. Default: `vmMonitoringDisabled`.
	HaVmMonitoring interface{}
	// Additional delay in seconds
	// after ready condition is met. A VM is considered ready at this point.
	// Default: `0` (no delay). <sup>\*</sup>
	HaVmRestartAdditionalDelay interface{}
	// The default restart priority
	// for affected virtual machines when vSphere detects a host failure. Can be one
	// of `lowest`, `low`, `medium`, `high`, or `highest`. Default: `medium`.
	HaVmRestartPriority interface{}
	// The maximum time, in seconds,
	// that vSphere HA will wait for virtual machines in one priority to be ready
	// before proceeding with the next priority. Default: `600` (10 minutes).
	// <sup>\*</sup>
	HaVmRestartTimeout interface{}
	// The timeout for each host maintenance mode
	// operation when removing hosts from a cluster. The value is specified in
	// seconds. Default: `3600` (1 hour).
	HostClusterExitTimeout interface{}
	// The [managed object IDs][docs-about-morefs] of
	// the hosts to put in the cluster.
	HostSystemIds interface{}
	// The name of the cluster.
	Name interface{}
	// Determines how the host
	// quarantine, maintenance mode, or virtual machine migration recommendations
	// made by proactive HA are to be handled. Can be one of `Automated` or
	// `Manual`. Default: `Manual`. <sup>\*</sup>
	ProactiveHaAutomationLevel interface{}
	// Enables Proactive HA. Default: `false`.
	// <sup>\*</sup>
	ProactiveHaEnabled interface{}
	// The configured remediation
	// for moderately degraded hosts. Can be one of `MaintenanceMode` or
	// `QuarantineMode`. Note that this cannot be set to `MaintenanceMode` when
	// `proactive_ha_severe_remediation` is set
	// to `QuarantineMode`. Default: `QuarantineMode`.
	// <sup>\*</sup>
	ProactiveHaModerateRemediation interface{}
	// The list of IDs for health update
	// providers configured for this cluster.
	// <sup>\*</sup>
	ProactiveHaProviderIds interface{}
	// The configured remediation for
	// severely degraded hosts. Can be one of `MaintenanceMode` or `QuarantineMode`.
	// Note that this cannot be set to `QuarantineMode` when
	// `proactive_ha_moderate_remediation` is
	// set to `MaintenanceMode`. Default: `QuarantineMode`.
	// <sup>\*</sup>
	ProactiveHaSevereRemediation interface{}
	// The managed object ID of the cluster's root resource pool.
	ResourcePoolId interface{}
	// The IDs of any tags to attach to this resource. See
	// [here][docs-applying-tags] for a reference on how to apply tags.
	Tags interface{}
}

// The set of arguments for constructing a ComputeCluster resource.
type ComputeClusterArgs struct {
	// A map of custom attribute ids to attribute
	// value strings to set for the datastore cluster. See
	// [here][docs-setting-custom-attributes] for a reference on how to set values
	// for custom attributes.
	CustomAttributes interface{}
	// The [managed object ID][docs-about-morefs] of
	// the datacenter to create the cluster in. Forces a new resource if changed.
	DatacenterId interface{}
	// The automation level for host power
	// operations in this cluster. Can be one of `manual` or `automated`. Default:
	// `manual`.
	DpmAutomationLevel interface{}
	// Enable DPM support for DRS in this cluster.
	// Requires `drs_enabled` to be `true` in order to be effective.
	// Default: `false`.
	DpmEnabled interface{}
	// A value between `1` and `5` indicating the
	// threshold of load within the cluster that influences host power operations.
	// This affects both power on and power off operations - a lower setting will
	// tolerate more of a surplus/deficit than a higher setting. Default: `3`.
	DpmThreshold interface{}
	// A key/value map that specifies advanced
	// options for DRS and DPM.
	DrsAdvancedOptions interface{}
	// The default automation level for all
	// virtual machines in this cluster. Can be one of `manual`,
	// `partiallyAutomated`, or `fullyAutomated`. Default: `manual`.
	DrsAutomationLevel interface{}
	// When `true`, enables DRS to use data
	// from [vRealize Operations Manager][ref-vsphere-vro] to make proactive DRS
	// recommendations. <sup>\*</sup>
	DrsEnablePredictiveDrs interface{}
	// Allow individual DRS overrides to be
	// set for virtual machines in the cluster. Default: `true`.
	DrsEnableVmOverrides interface{}
	// Enable DRS for this cluster. Default: `false`.
	DrsEnabled interface{}
	// A value between `1` and `5` indicating
	// the threshold of imbalance tolerated between hosts. A lower setting will
	// tolerate more imbalance while a higher setting will tolerate less. Default:
	// `3`.
	DrsMigrationThreshold interface{}
	// The relative path to a folder to put this cluster in.
	// This is a path relative to the datacenter you are deploying the cluster to.
	// Example: for the `dc1` datacenter, and a provided `folder` of `foo/bar`,
	// Terraform will place a cluster named `terraform-compute-cluster-test` in a
	// host folder located at `/dc1/host/foo/bar`, with the final inventory path
	// being `/dc1/host/foo/bar/terraform-datastore-cluster-test`.
	Folder interface{}
	// When destroying the resource, setting this to
	// `true` will auto-remove any hosts that are currently a member of the cluster,
	// as if they were removed by taking their entry out of `host_system_ids` (see
	// below). This is an advanced
	// option and should only be used for testing. Default: `false`.
	ForceEvacuateOnDestroy interface{}
	// Defines the
	// [managed object IDs][docs-about-morefs] of hosts to use as dedicated failover
	// hosts. These hosts are kept as available as possible - admission control will
	// block access to the host, and DRS will ignore the host when making
	// recommendations.
	HaAdmissionControlFailoverHostSystemIds interface{}
	// The maximum number
	// of failed hosts that admission control tolerates when making decisions on
	// whether to permit virtual machine operations. The maximum is one less than
	// the number of hosts in the cluster. Default: `1`.
	// <sup>\*</sup>
	HaAdmissionControlHostFailureTolerance interface{}
	// The percentage of
	// resource reduction that a cluster of virtual machines can tolerate in case of
	// a failover. A value of 0 produces warnings only, whereas a value of 100
	// disables the setting. Default: `100` (disabled).
	HaAdmissionControlPerformanceTolerance interface{}
	// The type of admission control
	// policy to use with vSphere HA. Can be one of `resourcePercentage`,
	// `slotPolicy`, `failoverHosts`, or `disabled`. Default: `resourcePercentage`.
	HaAdmissionControlPolicy interface{}
	// 
	// Automatically determine available resource percentages by subtracting the
	// average number of host resources represented by the
	// `ha_admission_control_host_failure_tolerance`
	// setting from the total amount of resources in the cluster. Disable to supply
	// user-defined values. Default: `true`.
	// <sup>\*</sup>
	HaAdmissionControlResourcePercentageAutoCompute interface{}
	// Controls the
	// user-defined percentage of CPU resources in the cluster to reserve for
	// failover. Default: `100`.
	HaAdmissionControlResourcePercentageCpu interface{}
	// Controls the
	// user-defined percentage of memory resources in the cluster to reserve for
	// failover. Default: `100`.
	HaAdmissionControlResourcePercentageMemory interface{}
	// Controls the
	// user-defined CPU slot size, in MHz. Default: `32`.
	HaAdmissionControlSlotPolicyExplicitCpu interface{}
	// Controls the
	// user-defined memory slot size, in MB. Default: `100`.
	HaAdmissionControlSlotPolicyExplicitMemory interface{}
	// Controls
	// whether or not you wish to supply explicit values to CPU and memory slot
	// sizes. The default is `false`, which tells vSphere to gather a automatic
	// average based on all powered-on virtual machines currently in the cluster.
	HaAdmissionControlSlotPolicyUseExplicitSize interface{}
	// A key/value map that specifies advanced
	// options for vSphere HA.
	HaAdvancedOptions interface{}
	// Controls the action to take
	// on virtual machines if an APD status on an affected datastore clears in the
	// middle of an APD event. Can be one of `none` or `reset`. Default: `none`.
	// <sup>\*</sup>
	HaDatastoreApdRecoveryAction interface{}
	// Controls the action to take on
	// virtual machines when the cluster has detected loss to all paths to a
	// relevant datastore. Can be one of `disabled`, `warning`,
	// `restartConservative`, or `restartAggressive`.  Default: `disabled`.
	// <sup>\*</sup>
	HaDatastoreApdResponse interface{}
	// Controls the delay in minutes
	// to wait after an APD timeout event to execute the response action defined in
	// `ha_datastore_apd_response`. Default: `3`
	// minutes. <sup>\*</sup>
	HaDatastoreApdResponseDelay interface{}
	// Controls the action to take on
	// virtual machines when the cluster has detected a permanent device loss to a
	// relevant datastore. Can be one of `disabled`, `warning`, or
	// `restartAggressive`. Default: `disabled`.
	// <sup>\*</sup>
	HaDatastorePdlResponse interface{}
	// Enable vSphere HA for this cluster. Default:
	// `false`.
	HaEnabled interface{}
	// The list of managed object IDs for
	// preferred datastores to use for HA heartbeating. This setting is only useful
	// when `ha_heartbeat_datastore_policy` is set
	// to either `userSelectedDs` or `allFeasibleDsWithUserPreference`.
	HaHeartbeatDatastoreIds interface{}
	// The selection policy for HA
	// heartbeat datastores. Can be one of `allFeasibleDs`, `userSelectedDs`, or
	// `allFeasibleDsWithUserPreference`. Default:
	// `allFeasibleDsWithUserPreference`.
	HaHeartbeatDatastorePolicy interface{}
	// The action to take on virtual
	// machines when a host has detected that it has been isolated from the rest of
	// the cluster. Can be one of `none`, `powerOff`, or `shutdown`. Default:
	// `none`.
	HaHostIsolationResponse interface{}
	// Global setting that controls whether
	// vSphere HA remediates virtual machines on host failure. Can be one of `enabled`
	// or `disabled`. Default: `enabled`.
	HaHostMonitoring interface{}
	// Controls vSphere VM component
	// protection for virtual machines in this cluster. Can be one of `enabled` or
	// `disabled`. Default: `enabled`.
	// <sup>\*</sup>
	HaVmComponentProtection interface{}
	// The condition used to
	// determine whether or not virtual machines in a certain restart priority class
	// are online, allowing HA to move on to restarting virtual machines on the next
	// priority. Can be one of `none`, `poweredOn`, `guestHbStatusGreen`, or
	// `appHbStatusGreen`. The default is `none`, which means that a virtual machine
	// is considered ready immediately after a host is found to start it on.
	// <sup>\*</sup>
	HaVmDependencyRestartCondition interface{}
	// If a heartbeat from a virtual machine
	// is not received within this configured interval, the virtual machine is
	// marked as failed. The value is in seconds. Default: `30`.
	HaVmFailureInterval interface{}
	// The length of the reset window in
	// which `ha_vm_maximum_resets` can operate. When this
	// window expires, no more resets are attempted regardless of the setting
	// configured in `ha_vm_maximum_resets`. `-1` means no window, meaning an
	// unlimited reset time is allotted. The value is specified in seconds. Default:
	// `-1` (no window).
	HaVmMaximumFailureWindow interface{}
	// The maximum number of resets that HA will
	// perform to a virtual machine when responding to a failure event. Default: `3`
	HaVmMaximumResets interface{}
	// The time, in seconds, that HA waits after
	// powering on a virtual machine before monitoring for heartbeats. Default:
	// `120` (2 minutes).
	HaVmMinimumUptime interface{}
	// The type of virtual machine monitoring to use
	// when HA is enabled in the cluster. Can be one of `vmMonitoringDisabled`,
	// `vmMonitoringOnly`, or `vmAndAppMonitoring`. Default: `vmMonitoringDisabled`.
	HaVmMonitoring interface{}
	// Additional delay in seconds
	// after ready condition is met. A VM is considered ready at this point.
	// Default: `0` (no delay). <sup>\*</sup>
	HaVmRestartAdditionalDelay interface{}
	// The default restart priority
	// for affected virtual machines when vSphere detects a host failure. Can be one
	// of `lowest`, `low`, `medium`, `high`, or `highest`. Default: `medium`.
	HaVmRestartPriority interface{}
	// The maximum time, in seconds,
	// that vSphere HA will wait for virtual machines in one priority to be ready
	// before proceeding with the next priority. Default: `600` (10 minutes).
	// <sup>\*</sup>
	HaVmRestartTimeout interface{}
	// The timeout for each host maintenance mode
	// operation when removing hosts from a cluster. The value is specified in
	// seconds. Default: `3600` (1 hour).
	HostClusterExitTimeout interface{}
	// The [managed object IDs][docs-about-morefs] of
	// the hosts to put in the cluster.
	HostSystemIds interface{}
	// The name of the cluster.
	Name interface{}
	// Determines how the host
	// quarantine, maintenance mode, or virtual machine migration recommendations
	// made by proactive HA are to be handled. Can be one of `Automated` or
	// `Manual`. Default: `Manual`. <sup>\*</sup>
	ProactiveHaAutomationLevel interface{}
	// Enables Proactive HA. Default: `false`.
	// <sup>\*</sup>
	ProactiveHaEnabled interface{}
	// The configured remediation
	// for moderately degraded hosts. Can be one of `MaintenanceMode` or
	// `QuarantineMode`. Note that this cannot be set to `MaintenanceMode` when
	// `proactive_ha_severe_remediation` is set
	// to `QuarantineMode`. Default: `QuarantineMode`.
	// <sup>\*</sup>
	ProactiveHaModerateRemediation interface{}
	// The list of IDs for health update
	// providers configured for this cluster.
	// <sup>\*</sup>
	ProactiveHaProviderIds interface{}
	// The configured remediation for
	// severely degraded hosts. Can be one of `MaintenanceMode` or `QuarantineMode`.
	// Note that this cannot be set to `QuarantineMode` when
	// `proactive_ha_moderate_remediation` is
	// set to `MaintenanceMode`. Default: `QuarantineMode`.
	// <sup>\*</sup>
	ProactiveHaSevereRemediation interface{}
	// The IDs of any tags to attach to this resource. See
	// [here][docs-applying-tags] for a reference on how to apply tags.
	Tags interface{}
}
